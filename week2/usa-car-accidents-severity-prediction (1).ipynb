{"cells":[{"metadata":{"id":"gMuY_peKPqJx"},"cell_type":"markdown","source":"# USA Car Accidents Severity Prediction\n\nby Jinz Wang\n\nMay 29, 2020\n\n","execution_count":null},{"metadata":{"id":"B6KKV32iPvag"},"cell_type":"markdown","source":"# 0 INTRODUCTION\n\n### Motivation\n\nThe economic and societal impact of traffic accidents cost U.S. citizens hundreds of billions of dollars every year. And a large part of losses is caused by a small number of serious accidents. Reducing traffic accidents, especially serious accidents, is nevertheless always an important challenge. The proactive approach, one of the two main approaches for dealing with traffic safety problems, focuses on preventing potential unsafe road conditions from occurring in the first place. For the effective implementation of this approach, accident prediction and severity prediciton are critical.\nIf we can identify the patterns of how these serious accidents happen and the key factors, we might be able to implement well-informed actions and better allocate financial and human resources. \n\n### Objectives\n\nThe first objective of this project is to recognize **key factors affecting the accident severity**. The second one is to develop a model that can **accurately predict accident severity**. To be specific, for a given accident, without any detailed information about itself, like driver attributes or vehicle type, this model is supposed to be able to predict the likelihood of this accident being a severe one. The accident could be the one that just happened and still lack of detailed information, or a potential one predicted by other models. Therefore, with the sophisticated real-time traffic accident prediction solution developed by the creators of the same dataset used in this project, this model might be able to further predict severe accidents in real-time.\n\n### Process\n\nData cleaning was first performed to detect and handle corrupt or missing records. EDA (Exploratory Data Analysis) and feature engineering were then done over most features. Finally, Logistic regression, Decision Tree Classifier, and Random Forest Classifier were used to develop the predictive model. The final model achieved **98.0% test accuracy** on resampled data.  \n\nIt is worth noting that the severity in this project is \"**an indication of the effect the accident has on traffic**\", rather than the injury severity that has already been thoroughly studied by many articles. Another thing is that the final model is dependent on only **a small range of data attributes** that are **easily achievable** for all regions in the United States and before the accident really happened. \n\n### Key Findings\n* Country-wide accident severity can be accurately predicted with limited data attributes (location, time, weather, and POI).\n* Spatial patterns are the most useful features. For small areas like **street** and **zipcode**, severe accidents are more likely to happen at places having more accidents while for larger areas like **city** and **airport region**, at places having less accident.\n* Time series features are also very important, especially **minute** in a day. An accident is more likely to be a serious one when accidents happen less frequently at this time. \n* If an accident happens on **Interstate Highway**, there is a 2% chance that it will be a serious one, which is about 2.3 times of average and higher than any other street type. \n* An accident is much less likely to be severe if it happens near **traffic signal** while more likely if near **junction**.\n* Weather features like **pressure**, **temperature**, **humidity**, and **wind speed** are also very important.\n\n### Dataset Overview\n\nUS-Accident dataset is a countrywide car accident dataset, which covers **49 states of the United States**. It contains **3 million cases** of traffic accidents that took place from **February 2016 to December 2019**. In this project, however, only the data of accidents that happened after **February 2019** and were reported by *MapQuest* was finally used in exploration analysis and modeling so that irrelevant factors can be eliminated to the greatest extent.\n\nLink for kaggle dataset: https://www.kaggle.com/sobhanmoosavi/us-accidents\n\n### Acknowledgements\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. “<a href=\"https://arxiv.org/abs/1906.05409\">A Countrywide Traffic Accident Dataset.</a>”, 2019.\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"<a href=\"https://arxiv.org/abs/1909.09638\">Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.</a>\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n\n### Refrences\n\nI found these notebooks really helpful:\n\n<a href=\"https://towardsdatascience.com/usa-accidents-data-analysis-d130843cde02\">USA Accidents Data Analysis</a>\n\nhttps://www.kaggle.com/sobhanmoosavi/us-accidents/discussion/113055\n\n<a href=\"https://www.kaggle.com/deepakdeepu8978/how-severity-the-accidents-is\">how Severity the Accidents is ?</a>\n\n<a href=\"https://www.kaggle.com/trivenisaraswathi/severity-prediction-in-sfo-bay-area\">Severity Prediction in SFO Bay Area</a> \n\n<a href=\"https://www.kaggle.com/phip2014/ml-to-predict-accident-severity-pa-mont\"> ML to Predict Accident Severity_PA_Mont</a>\n\n<a href=\"https://www.kaggle.com/suyash0010/severity-and-time-wasted-analysis\"> severity and hours wasted</a>\n\n<a href=\"https://www.kaggle.com/nikitagrec/usa-accidents-plotly-maps-text-classification\"> USA Accidents Plotly maps + text classification </a>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Tabel of content\n1. [OVERVIEW & PREPROCESSING](#1) \n    \n    1.1 [Overview](#1.1) \n    \n    1.2 [Reporting Source](#1.2)\n    \n    1.3 [Useless Features](#1.3)\n    \n    1.4 [Clean Up Categorical Features](#1.4)\n    \n    1.5 [Fix Datetime Format](#1.5)\n    \n    <br>    \n2. [HANDLING MISSING DATA](#2)\n\n    2.1 [Drop Features](#2.1)\n    \n    2.2 [Separate Feature](#2.2)\n    \n    2.3 [Drop NaN](#2.3)\n    \n    2.4 [Value Imputation](#2.4)\n    \n    <br>    \n3. [EXPLORATION & ENGINEERING](#3)\n    \n    3.1 [Resampling](#3.1)\n    \n    3.2 [Time Features](#3.2)\n    \n    3.3 [Address Features](#3.3)\n    \n    3.4 [Weather Features](#3.4)\n    \n    3.5 [POI Features](#3.5)\n    \n    3.6 [Correlation](#3.6)\n    \n    3.7 [One-hot Encoding](#3.7)\n    \n    <br>\n4. [MODEL](#4)\n    \n    4.1 [Train Test Split](#4.1)\n    \n    4.2 [Logistic Regression](#4.2)\n    \n    4.3 [Decision Tree](#4.3)\n    \n    4.4 [Random Forest](#4.4)  \n    \n    <br>\n5. [FUTURE WORK](#5)","execution_count":null},{"metadata":{"id":"-FDro7QKP3Mi"},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# 1 OVERVIEW & PREPROCESSING","execution_count":null},{"metadata":{"id":"cGX0rWqJeGmx","outputId":"7edd7fb7-31e9-4c36-b595-62017c3a67fd","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport json\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom datetime import datetime\nimport glob\nimport seaborn as sns\nimport re\nimport os\nimport io\nfrom scipy.stats import boxcox","execution_count":null,"outputs":[]},{"metadata":{"id":"Nd9HHY7NodZg"},"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n## 1.1 Overview the dataset\nDetails about features in the dataset:\n\n**Traffic Attributes (12)**:\n\n* **ID**: This is a unique identifier of the accident record.\n\n* **Source**: Indicates source of the accident report (i.e. the API which reported the accident.).\n\n* **TMC**: A traffic accident may have a Traffic Message Channel (TMC) code which provides more detailed description of the event.\n\n* **Severity**: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\n\n* **Start_Time**: Shows start time of the accident in local time zone.\n\n* **End_Time**: Shows end time of the accident in local time zone.\n\n* **Start_Lat**: Shows latitude in GPS coordinate of the start point.\n\n* **Start_Lng**: Shows longitude in GPS coordinate of the start point.\n\n* **End_Lat**: Shows latitude in GPS coordinate of the end point.\n\n* **End_Lng**: Shows longitude in GPS coordinate of the end point.\n\n* **Distance(mi)**: The length of the road extent affected by the accident.\n\n* **Description**: Shows natural language description of the accident.\n\n**Address Attributes (9)**:\n\n* **Number**: Shows the street number in address field.\n\n* **Street**: Shows the street name in address field.\n\n* **Side**: Shows the relative side of the street (Right/Left) in address field.\n\n* **City**: Shows the city in address field.\n\n* **County**: Shows the county in address field.\n\n* **State**: Shows the state in address field.\n\n* **Zipcode**: Shows the zipcode in address field.\n\n* **Country**: Shows the country in address field.\n\n* **Timezone**: Shows timezone based on the location of the accident (eastern, central, etc.).\n\n**Weather Attributes (11)**:\n\n* **Airport_Code**: Denotes an airport-based weather station which is the closest one to location of the accident.\n\n* **Weather_Timestamp**: Shows the time-stamp of weather observation record (in local time).\n\n* **Temperature(F)**: Shows the temperature (in Fahrenheit).\n\n* **Wind_Chill(F)**: Shows the wind chill (in Fahrenheit).\n\n* **Humidity(%)**: Shows the humidity (in percentage).\n\n* **Pressure(in)**: Shows the air pressure (in inches).\n\n* **Visibility(mi)**: Shows visibility (in miles).\n\n* **Wind_Direction**: Shows wind direction.\n\n* **Wind_Speed(mph)**: Shows wind speed (in miles per hour).\n\n* **Precipitation(in)**: Shows precipitation amount in inches, if there is any.\n\n* **Weather_Condition**: Shows the weather condition (rain, snow, thunderstorm, fog, etc.).\n\n**POI Attributes (13)**:\n\n* **Amenity**: A Point-Of-Interest (POI) annotation which indicates presence of amenity in a nearby location.\n\n* **Bump**: A POI annotation which indicates presence of speed bump or hump in a nearby location.\n\n* **Crossing**: A POI annotation which indicates presence of crossing in a nearby location.\n\n* **Give_Way**: A POI annotation which indicates presence of give_way sign in a nearby location.\n\n* **Junction**: A POI annotation which indicates presence of junction in a nearby location.\n\n* **No_Exit**: A POI annotation which indicates presence of no_exit sign in a nearby location.\n\n* **Railway**: A POI annotation which indicates presence of railway in a nearby location.\n\n* **Roundabout**: A POI annotation which indicates presence of roundabout in a nearby location.\n\n* **Station**: A POI annotation which indicates presence of station (bus, train, etc.) in a nearby location.\n\n* **Stop**: A POI annotation which indicates presence of stop sign in a nearby location.\n\n* **Traffic_Calming**: A POI annotation which indicates presence of traffic_calming means in a nearby location.\n\n* **Traffic_Signal**: A POI annotation which indicates presence of traffic_signal in a nearby location.\n\n* **Turning_Loop**: A POI annotation which indicates presence of turning_loop in a nearby location.\n\n**Period-of-Day (4)**:\n\n* **Sunrise_Sunset**: Shows the period of day (i.e. day or night) based on sunrise/sunset.\n\n* **Civil_Twilight**: Shows the period of day (i.e. day or night) based on civil twilight.\n\n* **Nautical_Twilight**: Shows the period of day (i.e. day or night) based on nautical twilight.\n\n* **Astronomical_Twilight**: Shows the period of day (i.e. day or night) based on astronomical twilight.","execution_count":null},{"metadata":{"id":"CE3iYqcWfee8","outputId":"023a665d-cc4c-4a98-8a06-03da57a406e3","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/us-accidents/US_Accidents_Dec19.csv')\nprint(\"The shape of data is:\",(df.shape))\ndisplay(df.head(3))","execution_count":null,"outputs":[]},{"metadata":{"id":"mkasxxejZR_f"},"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n## 1.2 Reporting Sources\n    \nThese data came from two sources, *MapQuest* and *Bing*, both of which report severity level but in a different way. Bing has 4 levels while MapQuest has 5. And according to dataset creator, there is no way to do a 1:1 mapping between them. Since severity is what we really care about in this project, I think it is crucial to figure out the difference.","execution_count":null},{"metadata":{"id":"DhfBuyDJZlVH","outputId":"af0b5ccf-275f-4e9e-f70b-00640060a0ed","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"df_source = df.groupby(['Severity','Source']).size().reset_index().pivot(\\\n    columns='Severity', index='Source', values=0)\ndf_source.plot(kind='bar', stacked=True, title='Severity Count by Sources')","execution_count":null,"outputs":[]},{"metadata":{"id":"YDnD7CM92pP1"},"cell_type":"markdown","source":"The stacked bar chart shows that two data providers reported totally different proportions of accidents of each level. *MapQuest* reported so rare accidents with severity level 4 which can not even be seen in the plot, whereas *Bing* reported almost the same number of level 4 accidents as level 2. Meanwhile, *MapQuest* reported much more level 3 accidents than *Bing* in terms of proportion. These differences may be due to the different kinds of accidents they tend to collect or the different definitions of severity level, or the combination of them. If the latter is the case, I don't think we can use the data from both of them at the same time. To check it out, we can examine the distribution of accidents with different severity levels across two main measures, **Impacted Distance** and **Duration**.\n","execution_count":null},{"metadata":{"id":"1RW8ZmbJ9o1i","outputId":"d6794e52-62fd-42a8-f163-f757bbe8afba","trusted":true},"cell_type":"code","source":"# fix datetime type\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'])\ndf['End_Time'] = pd.to_datetime(df['End_Time'])\ndf['Weather_Timestamp'] = pd.to_datetime(df['Weather_Timestamp'])\n\n# calculate duration as the difference between end time and start time in minute\ndf['Duration'] = df.End_Time - df.Start_Time \ndf['Duration'] = df['Duration'].apply(lambda x:round(x.total_seconds() / 60) )\nprint(\"The overall mean duration is: \", (round(df['Duration'].mean(),3)), 'min')","execution_count":null,"outputs":[]},{"metadata":{"id":"eIYOiFXzCigC","outputId":"0b047680-9164-4b4b-83df-207c1fb080ee","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\nsns.boxplot(x=\"Severity\", y=\"Duration\",\n            data=df.loc[(df['Source']==\"MapQuest\") & (df['Duration']<400),], palette=\"Set2\", ax=axs[0])\naxs[0].set_title('MapQuest')\nfig.suptitle('Accidents Duration by Severity', fontsize=16)\nsns.boxplot(x=\"Severity\", y=\"Duration\",\n            data=df.loc[(df['Source']==\"Bing\") & (df['Duration']<400),], palette=\"Set2\", ax=axs[1])\naxs[1].set_title('Bing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"jimdbmOEIByk","outputId":"f0a190f0-e30a-493e-eff9-34faa9059b48","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\nsns.boxplot(x=\"Severity\", y=\"Distance(mi)\", \n            data=df.loc[(df['Source']==\"MapQuest\") & (df['Distance(mi)']<5),], palette=\"Set2\", ax=axs[0])\naxs[0].set_title('MapQuest')\nfig.suptitle('Impacted Distance by Severity', fontsize=16)\nsns.boxplot(x=\"Severity\", y=\"Distance(mi)\",\n            data=df.loc[(df['Source']==\"Bing\") & (df['Distance(mi)']<5),], palette=\"Set2\", ax=axs[1])\naxs[1].set_title('Bing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"MK0xcM50oQoX"},"cell_type":"markdown","source":"Two differences are obvious in the above plots. The first is that the overall duration and impacted distance of accidents reported by *Bing* are much longer than those by *MapQuest*. Second, same severity level holds different meanings for *MapQuest* and *Bing*. *MapQuest* seems to have a clear and strict threshold for severity level 4, cases of which nevertheless only account for a tiny part of the whole dataset. *Bing*, on the other hand, doesn't seem to have a clear-cut threshold, especially regards duration, but the data is more balanced. \n\nIt is hard to choose one and we definitely can't use both. I decided to select *MapQuest* because serious accidents are we really care about and the sparse data of such accidents is the reality we have to confront.\n\nFinally, drop data reported from *Bing* and 'Source' column.","execution_count":null},{"metadata":{"id":"h8xBDgrlcBGT","trusted":true},"cell_type":"code","source":"df = df.loc[df['Source']==\"MapQuest\",]\ndf = df.drop(['Source'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"fg4ecO7DQO0K"},"cell_type":"markdown","source":"<a id=\"1.3\"></a>    \n## 1.3 Useless Features\nFeatures 'ID' doesn't provide any useful information about accidents themselves. 'TMC', 'Distance(mi)', 'End_Time' (we have start time), 'Duration', 'End_Lat', and 'End_Lng'(we have start location) can be collected only after the accident has already happened and hence cannot be predictors for serious accident prediction. For 'Description', the POI features have already been extracted from it by dataset creators. Let's get rid of these features first.\n\n","execution_count":null},{"metadata":{"id":"LIb_0DTxnWzt","trusted":true},"cell_type":"code","source":"df = df.drop(['ID','TMC','Description','Distance(mi)', 'End_Time', 'Duration', \n              'End_Lat', 'End_Lng'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"G6ZmltRz3tLP"},"cell_type":"markdown","source":"Check out some categorical features.","execution_count":null},{"metadata":{"id":"gn8W2FkF2jd7","outputId":"42d78206-89a0-489e-c533-d4574b08813a","trusted":true},"cell_type":"code","source":"cat_names = ['Side', 'Country', 'Timezone', 'Amenity', 'Bump', 'Crossing', \n             'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', \n             'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', \n             'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\nprint(\"Unique count of categorical features:\")\nfor i in cat_names:\n  print(i,df[i].unique().size)","execution_count":null,"outputs":[]},{"metadata":{"id":"PM2Uekhh36uZ"},"cell_type":"markdown","source":"Drop 'Country' and 'Turning_Loop' for they have only one class.","execution_count":null},{"metadata":{"id":"48IdU8YS4Nrz","trusted":true},"cell_type":"code","source":"df = df.drop(['Country','Turning_Loop'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ywyh4XCN44BC"},"cell_type":"markdown","source":"<a id=\"1.4\"></a>\n## 1.4 Clean Up Categorical Features\nIf we look at categorical features closely, we will find some chaos in 'Wind_Direction' and 'Weather_Condition'. It is necessary to clean them up first.\n\n### Wind Direction","execution_count":null},{"metadata":{"id":"j0QP6fPKQc7Y","outputId":"cafcd815-6e9a-454c-eca8-da251fac01c8","trusted":true},"cell_type":"code","source":"print(\"Wind Direction: \", df['Wind_Direction'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"2Rtv3KsmG_QE"},"cell_type":"markdown","source":"Simplify wind direction","execution_count":null},{"metadata":{"id":"-LItmME1Qz9P","outputId":"f8d91f10-0c2d-4792-a985-6939cb063ef8","trusted":true},"cell_type":"code","source":"df.loc[df['Wind_Direction']=='Calm','Wind_Direction'] = 'CALM'\ndf.loc[(df['Wind_Direction']=='West')|(df['Wind_Direction']=='WSW')|(df['Wind_Direction']=='WNW'),'Wind_Direction'] = 'W'\ndf.loc[(df['Wind_Direction']=='South')|(df['Wind_Direction']=='SSW')|(df['Wind_Direction']=='SSE'),'Wind_Direction'] = 'S'\ndf.loc[(df['Wind_Direction']=='North')|(df['Wind_Direction']=='NNW')|(df['Wind_Direction']=='NNE'),'Wind_Direction'] = 'N'\ndf.loc[(df['Wind_Direction']=='East')|(df['Wind_Direction']=='ESE')|(df['Wind_Direction']=='ENE'),'Wind_Direction'] = 'E'\ndf.loc[df['Wind_Direction']=='Variable','Wind_Direction'] = 'VAR'\nprint(\"Wind Direction after simplification: \", df['Wind_Direction'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"WS-3z6XjI3bx"},"cell_type":"markdown","source":"### Weather Condition\nWeather-related vehicle accidents kill more people annually than large-scale weather disasters(source: weather.com). According to Road Weather Management Program, most weather-related crashes happen on wet-pavement and during rainfall. Winter-condition and fog are another two main reasons for weather-related accidents. To extract these three weather conditions, we first look at what we have in 'Weather_Condition' Feature.\n","execution_count":null},{"metadata":{"id":"x3A6tH88CFiv","outputId":"fb822847-f6f0-4b6f-cd05-7f1d52c1d5ad","trusted":true},"cell_type":"code","source":"# show distinctive weather conditions \nweather ='!'.join(df['Weather_Condition'].dropna().unique().tolist())\nweather = np.unique(np.array(re.split(\n    \"!|\\s/\\s|\\sand\\s|\\swith\\s|Partly\\s|Mostly\\s|Blowing\\s|Freezing\\s\", weather))).tolist()\nprint(\"Weather Conditions: \", weather)","execution_count":null,"outputs":[]},{"metadata":{"id":"GOM2GlthCFiy"},"cell_type":"markdown","source":"Create features for some common weather conditions and drop 'Weather_Condition' then.","execution_count":null},{"metadata":{"id":"-5xg-h1oM0ap","trusted":true},"cell_type":"code","source":"df['Clear'] = np.where(df['Weather_Condition'].str.contains('Clear', case=False, na = False), 1, 0)\ndf['Cloud'] = np.where(df['Weather_Condition'].str.contains('Cloud|Overcast', case=False, na = False), 1, 0)\ndf['Rain'] = np.where(df['Weather_Condition'].str.contains('Rain|storm', case=False, na = False), 1, 0)\ndf['Heavy_Rain'] = np.where(df['Weather_Condition'].str.contains('Heavy Rain|Rain Shower|Heavy T-Storm|Heavy Thunderstorms', case=False, na = False), 1, 0)\ndf['Snow'] = np.where(df['Weather_Condition'].str.contains('Snow|Sleet|Ice', case=False, na = False), 1, 0)\ndf['Heavy_Snow'] = np.where(df['Weather_Condition'].str.contains('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls', case=False, na = False), 1, 0)\ndf['Fog'] = np.where(df['Weather_Condition'].str.contains('Fog', case=False, na = False), 1, 0)\n\n# Assign NA to created weather features where 'Weather_Condition' is null.\nweather = ['Clear','Cloud','Rain','Heavy_Rain','Snow','Heavy_Snow','Fog']\nfor i in weather:\n  df.loc[df['Weather_Condition'].isnull(),i] = df.loc[df['Weather_Condition'].isnull(),'Weather_Condition']\n\ndf.loc[:,['Weather_Condition'] + weather]\n\ndf = df.drop(['Weather_Condition'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ByDeO5PHZqMx"},"cell_type":"markdown","source":"<a id=\"1.5\"></a>\n## 1.5 Fix Datetime Format","execution_count":null},{"metadata":{"id":"Kpxt_66DRAyM","outputId":"f0ecd317-a869-461c-de26-f4912efbebe4","trusted":true},"cell_type":"code","source":"# average difference between weather time and start time\nprint(\"Mean difference between 'Start_Time' and 'Weather_Timestamp': \", \n(df.Weather_Timestamp - df.Start_Time).mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"zBn975dDUv4h"},"cell_type":"markdown","source":"Since the 'Weather_Timestamp' is almost as same as 'Start_Time', we can just keep 'Start_Time'. Then map 'Start_Time' to 'Year', 'Month', 'Weekday', 'Day' (in a year), 'Hour', and 'Minute' (in a day).","execution_count":null},{"metadata":{"id":"0W4CubmvSobC","trusted":true},"cell_type":"code","source":"df = df.drop([\"Weather_Timestamp\"], axis=1)\n\ndf['Year'] = df['Start_Time'].dt.year\n\nnmonth = df['Start_Time'].dt.month\ndf['Month'] = nmonth\n\ndf['Weekday']= df['Start_Time'].dt.weekday\n\ndays_each_month = np.cumsum(np.array([0,31,28,31,30,31,30,31,31,30,31,30,31]))\nnday = [days_each_month[arg-1] for arg in nmonth.values]\nnday = nday + df[\"Start_Time\"].dt.day.values\ndf['Day'] = nday\n\ndf['Hour'] = df['Start_Time'].dt.hour\n\ndf['Minute']=df['Hour']*60.0+df[\"Start_Time\"].dt.minute\n\ndf.loc[:4,['Start_Time', 'Year', 'Month', 'Weekday', 'Day', 'Hour', 'Minute']]","execution_count":null,"outputs":[]},{"metadata":{"id":"Cte34NibRp6-"},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# 2 HANDLING MISSING DATA\n<a id=\"2.1\"></a>\n## 2.1 Drop Features\nAs seen from below, many columns have missing values.","execution_count":null},{"metadata":{"id":"52X0bnvW3Vtz","outputId":"b2f26b4c-035c-4419-843e-7baa1eade1d4","trusted":true},"cell_type":"code","source":"missing = pd.DataFrame(df.isnull().sum()).reset_index()\nmissing.columns = ['Feature', 'Missing_Percent(%)']\nmissing['Missing_Percent(%)'] = missing['Missing_Percent(%)'].apply(lambda x: x / df.shape[0] * 100)\nmissing.loc[missing['Missing_Percent(%)']>0,:]","execution_count":null,"outputs":[]},{"metadata":{"id":"hjilXcSHRx9d"},"cell_type":"markdown","source":"More than 60% percent of 'Number', 'Wind_Chill(F)', and 'Precipitation(in)' is missing. Drop na and value imputation wouldn't work for these features. 'Number' and 'Wind_Chill(F)' will be dropped because they are not highly related to severity according to previous research, whereas 'Precipitation(in)' could be a useful predictor and hence can be handled by separating feature.\n\nDrop these features:\n\n 1. 'Number'\n\n 2. 'Wind_Chill(F)'\n","execution_count":null},{"metadata":{"id":"YuLuGBWgqfQ4","trusted":true},"cell_type":"code","source":"df = df.drop(['Number','Wind_Chill(F)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"6-3m73DOzwL6"},"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n## 2.2 Separate Featrue\nAdd a new feature for missing values in 'Precipitation(in)' and replace missing values with median.\n","execution_count":null},{"metadata":{"id":"abJWi0CH3lhS","outputId":"9f46085f-ea93-4c70-f167-ff8a026a6ff3","trusted":true},"cell_type":"code","source":"df['Precipitation_NA'] = 0\ndf.loc[df['Precipitation(in)'].isnull(),'Precipitation_NA'] = 1\ndf['Precipitation(in)'] = df['Precipitation(in)'].fillna(df['Precipitation(in)'].median())\ndf.loc[:5,['Precipitation(in)','Precipitation_NA']]","execution_count":null,"outputs":[]},{"metadata":{"id":"jqf6aFT5SBjl"},"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n## 2.3 Drop NaN\nThe counts of missing values in some features are much smaller compared to the total sample. It is convenient to drop rows with missing values in these columns.\n\nDrop NAs by these features:\n\n1. 'City'\n2. 'Zipcode'\n3. 'Airport_Code'\n4. 'Sunrise_Sunset'\n5. 'Civil_Twilight'\n6. 'Nautical_Twilight'\n7. 'Astronomical_Twilight'","execution_count":null},{"metadata":{"id":"NfCAW37CSTEN","trusted":true},"cell_type":"code","source":"df = df.dropna(subset=['City','Zipcode','Airport_Code',\n                       'Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'])","execution_count":null,"outputs":[]},{"metadata":{"id":"oLMGINInpBv-"},"cell_type":"markdown","source":"<a id=\"2.4\"></a>\n## 2.4 Value Imputation\nMost of the rest columns only have small missing part that can be filled. (It is not absolutely necessary though, we can also just drop na)\n### Continuous Weather Data\nContinuous weather features with missing values:\n\n1. Temperature(F)\n\n2. Humidity(%)\n\n3. Pressure(in)\n\n4. Visibility(mi)\n\n5. Wind_Speed(mph)\n\nBefore imputation, weather features will be grouped by location and time first, to which weather is naturally related. 'Airport_Code' is selected as location feature because the sources of weather data are airport-based weather stations. Then the data will be grouped by 'Start_Month' rather than 'Start_Hour' because using the former is computationally cheaper and remains less missing values. Finally, missing values will be replaced by median value of each group. ","execution_count":null},{"metadata":{"id":"VeIOMx8UajKg","outputId":"befb401d-af36-4dc3-9c7c-e3e070bb8726","trusted":true},"cell_type":"code","source":"# group data by 'Airport_Code' and 'Start_Month' then fill NAs with median value\nWeather_data=['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\nprint(\"The number of remaining missing values: \")\nfor i in Weather_data:\n  df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(x.median()))\n  print( i + \" : \" + df[i].isnull().sum().astype(str))","execution_count":null,"outputs":[]},{"metadata":{"id":"r0QWSev3WMsG"},"cell_type":"markdown","source":"There still are some missing values but much less. Just dropna by these features for the sake of simplicity.","execution_count":null},{"metadata":{"id":"kpkoomnutwJG","trusted":true},"cell_type":"code","source":"df = df.dropna(subset=Weather_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"_jBsE-WDK5rN"},"cell_type":"markdown","source":"### Categorical Weather Features\nFor categorical weather features, majority rather than median will be used to replace missing values.","execution_count":null},{"metadata":{"id":"U6fmpxdft1-F","outputId":"9ac9fb53-1152-42e7-a646-e27089eda7df","trusted":true},"cell_type":"code","source":"# group data by 'Airport_Code' and 'Start_Month' then fill NAs with majority value\nfrom collections import Counter\nweather_cat = ['Wind_Direction'] + weather\nprint(\"Count of missing values that will be dropped: \")\nfor i in weather_cat:\n  df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\n  print(i + \" : \" + df[i].isnull().sum().astype(str))\n\n# drop na\ndf = df.dropna(subset=weather_cat)","execution_count":null,"outputs":[]},{"metadata":{"id":"BjzYMh0Nd0fg"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# 3 EXPLORATION & ENGINEERING\n<a id=\"3.1\"></a>\n## 3.1 Resampling\nBased on the exploration we did in 1.2, the accidents with severity level 4 are much more serious than accidents of other levels, between which the division is far from clear-cut. Therefore, I decided to focus on level 4 accidents and regroup the levels of severity into level 4 versus other levels. ","execution_count":null},{"metadata":{"id":"FTitVELQR67e","trusted":true},"cell_type":"code","source":"df['Severity4'] = 0\ndf.loc[df['Severity'] == 4, 'Severity4'] = 1\ndf.Severity4.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"W6hyPUqGWG0t"},"cell_type":"markdown","source":"As seen from above, the data is so unbalanced that we can hardly do exploratory analysis. To address this issue, the combination of over- and under-sampling will be used since the dataset is large enough. level 4 will be randomly oversampled to 100000 and other levels will be randomly undersampled to 100000.","execution_count":null},{"metadata":{"id":"yVDkjncEldBb","outputId":"f441b06a-6cdc-40a9-fb0e-f5a2a0701786","trusted":true},"cell_type":"code","source":"df = df.drop(['Severity'], axis = 1)\ndf_bl = pd.concat([df[df['Severity4']==1].sample(100000, replace = True),\n                   df[df['Severity4']==0].sample(100000)], axis=0)\nprint('resampled data:', df_bl.Severity4.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"id":"oQB8pBy5ZqS-"},"cell_type":"markdown","source":"Then we can do some exploratoty analysis on resampled data. ","execution_count":null},{"metadata":{"id":"0h4xDOnf63Sb"},"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n## 3.2 Time Features\n### Year","execution_count":null},{"metadata":{"id":"B0lzrFtP6zCD","outputId":"b3c76675-9d79-4b38-97e3-770e54423c5f","trusted":true},"cell_type":"code","source":"df_bl.Year = df_bl.Year.astype(str)\nsns.countplot(x='Year', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Year (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ie-Cqv-ra9H2"},"cell_type":"markdown","source":"There must be something wrong. It is impossible that the number of accidents with severity level 4 in 2019 is more than 5 times the number in 2018 while the number of other levels accidents is less. Let's back to raw data to have a look.\n\nI created a heatmap of accidents with severity level 4 from 2016 to 2019, seeing how they actually distributed.","execution_count":null},{"metadata":{"id":"5j1fgUuIFz0N","outputId":"849c1dd9-2ede-4de2-d3dc-7426254a3a39","trusted":true},"cell_type":"code","source":"# create a dataframe used to plot heatmap\ndf_date = df.loc[:,['Start_Time','Severity4']]         # create a new dateframe only containing time and severity\ndf_date['date'] = df_date['Start_Time'].dt.normalize() # keep only the date part of start time\ndf_date = df_date.drop(['Start_Time'], axis = 1)\ndf_date = df_date.groupby('date').sum()                # sum the number of accidents with severity level 4 by date\ndf_date = df_date.reset_index().drop_duplicates()\n\n# join the dataframe with full range of date from 2016 to 2019\nfull_date = pd.DataFrame(pd.date_range(start=\"2016-01-02\",end=\"2019-12-31\"))    \ndf_date = full_date.merge(df_date, how = 'left',left_on = 0, right_on = 'date')\ndf_date['date'] = df_date.iloc[:,0]\ndf_date = df_date.fillna(0)\ndf_date = df_date.iloc[:,1:].set_index('date')\n\n# group by date\ngroups = df_date['Severity4'].groupby(pd.Grouper(freq='A'))\nyears = pd.DataFrame()\nfor name, group in groups:\n  years[name.year] = group.values\n\n# plot\nyears = years.T\nplt.matshow(years, interpolation=None, aspect='auto')\nplt.title('Time Heatmap of Accident with Severity Level 4 (raw data)', y=1.2, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"RjeHj7add-H2"},"cell_type":"markdown","source":"The heatmap indicates that something changed after Feb 2019. Maybe it is the way that *MapQuest* defines severity or the way they collect data. Anyway, we have to narrow down our data again. Since the data after Feb 2019 is less imbalanced and the data in the future is more likely to look like this, dropping the data before Mar 2019 may be the best choice.","execution_count":null},{"metadata":{"id":"6lYaeUl1g-TU","trusted":true},"cell_type":"code","source":"df = df.loc[(df['Year']==2019) & (df['Month']!=1) & (df['Month']!=2),:]\ndf = df.drop(['Year', 'Start_Time'], axis=1)\ndf['Severity4'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"vWTsnFL9SDSF"},"cell_type":"markdown","source":"Resample cases of both level 4 and other levels accidents to 40000 this time.","execution_count":null},{"metadata":{"id":"agHVJxURh2rW","trusted":true},"cell_type":"code","source":"df_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), df[df['Severity4']==0].sample(40000)], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"n1JLhEaUu-bN"},"cell_type":"markdown","source":"\n### Month\nIt's quite interesting that the count of other levels accidents is mostly consistent from March to December, whereas the number of level 4 accidents rapidly increased from March to May and remained stable until September then increased again from October.   ","execution_count":null},{"metadata":{"id":"wo3PE-eS7SI6","outputId":"99709021-e8cc-4f17-cded-bd44f2c27c27","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='Month', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Month (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"A2zJRXR-wS00"},"cell_type":"markdown","source":"### Weekday\nThe number of accidents was much less on weekends while the proportion of level 4 accidents was higher.","execution_count":null},{"metadata":{"id":"kMd46kKr75Py","outputId":"1aac9d6a-8de2-4ec0-c537-3a7679456023","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='Weekday', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Weedday (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1BHaXShyQ0yA"},"cell_type":"markdown","source":"### Period-of-Day\nAccidents were less during the night but were more likely to be serious.","execution_count":null},{"metadata":{"id":"DylnDZzkk2hW","outputId":"8bad28a8-7337-45ff-bc4d-d522fe6586f7","trusted":true},"cell_type":"code","source":"period_features = ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']\nfig, axs = plt.subplots(ncols=1, nrows=4, figsize=(13, 5))\n\nplt.subplots_adjust(wspace = 0.5)\nfor i, feature in enumerate(period_features, 1):    \n    plt.subplot(1, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in\\n{} Feature'.format(feature), size=13, y=1.05)\nfig.suptitle('Count of Accidents by Period-of-Day (resampled data)',y=1.08, fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"yKXFEdpQxMAE"},"cell_type":"markdown","source":"### Hour\nMost accidents happened during the daytime, especially AM peak and PM peak. When it comes to night, accidents were far less but more likely to be serious.","execution_count":null},{"metadata":{"id":"-K20SSaw7me1","outputId":"92c1c0a7-c7cb-489a-ae96-5dfd7fdab898","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Hour', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Hour (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"GC8XVUM6GnNf"},"cell_type":"markdown","source":"### Frequence Encoding (Minute)\nAs seen in the plot of 'Hour', 'Minute' may also be an important predictor. But directly using it would produce an overabundance of dummy variables. Therefore, the frequency of 'Minute' was utilized as labels, rather than 'Minute' itself. To normalize the distribution, the frequency was also transformed by log.","execution_count":null},{"metadata":{"id":"uKj0MpTW5bQz","outputId":"685c1d2d-226d-4c30-98d5-af318b3b74ea","trusted":true},"cell_type":"code","source":"# frequence encoding and log-transform\ndf['Minute_Freq'] = df.groupby(['Minute'])['Minute'].transform('count')\ndf['Minute_Freq'] = df['Minute_Freq']/df.shape[0]*24*60\ndf['Minute_Freq'] = df['Minute_Freq'].apply(lambda x: np.log(x+1))\n\n# resampling\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\n# plot\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nsns.violinplot(x='Minute_Freq', y=\"Severity4\", data=df_bl, palette=\"Set2\")    \nplt.xlabel('Minute_Fre', size=12, labelpad=3)\nplt.ylabel('Severity4', size=12, labelpad=3)    \nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.title('Minute Frequency by Severity (resampled data)', size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"enLdOZ3rYo-f"},"cell_type":"markdown","source":"The violin plot shows that the overall minute frequency of accidents with severity level 4 is less than other levels. In other words, an accident is more likely to be a serious one when accidents happen less frequently.","execution_count":null},{"metadata":{"id":"MsGVra7C8pZS"},"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n## 3.3 Address Features\n### Timezone\nEastern time zone is the most dangeous one.","execution_count":null},{"metadata":{"id":"AthEO9l5gex0","outputId":"c551b221-a6eb-4374-93fd-631ba1b96b4b","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,5))\nchart = sns.countplot(x='Timezone', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents by Timezone (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"WYAmIQhdRtz9"},"cell_type":"markdown","source":"### State\nFl, CA, and TX are the top 3 states with the most accidents.","execution_count":null},{"metadata":{"id":"SRYW-Nmqz1Iz","outputId":"380343d1-bdc4-4544-b126-47e02fab8655","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nchart = sns.countplot(x='State', hue='Severity4', \n                      data=df_bl ,palette=\"Set2\", order=df_bl['State'].value_counts().index)\nplt.title(\"Count of Accidents in State\\nordered by accidents' count (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"cwwhKU2o33KP"},"cell_type":"markdown","source":"It is a different story if we order the plot by the count of accidents with severity of level 4. FL is still the top one but the next two are GA and VA.","execution_count":null},{"metadata":{"id":"j9PICtgp3NRY","outputId":"cb072421-c03f-43e2-e8f1-a440893e78b0","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nchart = sns.countplot(x='State', hue='Severity4', data=df_bl ,palette=\"Set2\", order=df_bl[df_bl['Severity4']==1]['State'].value_counts().index)\nplt.title(\"Count of Accidents in State\\nordered by serious accidents' count (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"0giNeHInR8qd"},"cell_type":"markdown","source":"### County\nThere are too many counties that we cannot visualize them as we did for states. But we do can incorporate census data for them.\n\nSeveral basic variables, like total population, percent of commuters who drive, take transit or walk to work, and median household income, for all counties were downloaded from ACS 5-year estimates 2018. Then, counties' names were isolated. ","execution_count":null},{"metadata":{"id":"ulvOGo0gWWSV","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install -q censusdata\nimport censusdata\n\n# download data\ncounty = censusdata.download('acs5', 2018, censusdata.censusgeo([('county', '*')]),\n                                   ['DP05_0001E',  'DP03_0019PE','DP03_0021PE','DP03_0022PE','DP03_0062E'],\n                                   tabletype='profile')\n# rename columns\ncounty.columns = ['Population_County','Drive_County','Transit_County','Walk_County','MedianHouseholdIncome_County']\ncounty = county.reset_index()\n# extract county name and convert them to lowercase\ncounty['County_y'] = county['index'].apply(lambda x : x.name.split(' County')[0].split(',')[0]).str.lower()","execution_count":null,"outputs":[]},{"metadata":{"id":"ulP40a2jhohn","outputId":"bca7d4a3-742b-438a-8cf0-9a24ff7b2bd0","trusted":true},"cell_type":"code","source":"county.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"tAL2tqbIjy-x"},"cell_type":"markdown","source":"Counties' names turned out to be very tricky. Converting all of them into lowercase is not enough. Some counties name in USA-accidents omit \"city\" or \"parish\", and hence can't be matched with names in census data. We need to manually put them back and rejoin them.","execution_count":null},{"metadata":{"id":"_zKZya6mlLcc","outputId":"4ed9c10e-0b6e-4417-a6f2-b9b784c156ae","trusted":true},"cell_type":"code","source":"# convert all county name to lowercase \ndf['County'] = df['County'].str.lower()\n\n# left join df with census data\ndf = df.merge(county, left_on = 'County', right_on='County_y',how = 'left').drop('County_y', axis = 1)\njoin_var = county.columns.to_list()[:-1]\n\n# check how many miss match we got\nprint('Count of missing values before: ', df[join_var].isnull().sum())\n\n# add \"city\" and match again\ndf_city = df[df['Walk_County'].isnull()].drop(join_var, axis=1)\ndf_city['County_city'] = df_city['County'].apply(lambda x : x + ' city')\ndf_city = df_city.merge(county,left_on= 'County_city',right_on = 'County_y', how = 'left').drop(['County_city','County_y'], axis=1)\ndf = pd.concat((df[df['Walk_County'].isnull()==False], df_city), axis=0)\n\n# add \"parish\" and match again\ndf_parish = df[df['Walk_County'].isnull()].drop(join_var, axis=1)\ndf_parish['County_parish'] = df_parish['County'].apply(lambda x : x + ' parish')\ndf_parish = df_parish.merge(county,left_on= 'County_parish',right_on = 'County_y', how = 'left').drop(['County_parish','County_y'], axis=1)\ndf = pd.concat((df[df['Walk_County'].isnull()==False], df_parish), axis=0)\nprint('Count of missing values after: ', df[join_var].isnull().sum())\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kjgOh3P0raIN"},"cell_type":"markdown","source":"Drop na and use Logit transformation on some variables having extremly skewed distribution.","execution_count":null},{"metadata":{"id":"wQ2-sM4bmZnC","outputId":"8d15bf74-9ac2-490d-b3b6-d6d4b531d520","trusted":true},"cell_type":"code","source":"# drop na\ndf = df.drop('index', axis = 1).dropna()\n\n# log-transform\nfor i in ['Population_County','Transit_County','Walk_County']:\n    df[i + '_log'] = df[i].apply(lambda x: np.log(x+1))\ndf = df.drop(['Population_County','Transit_County','Walk_County'], axis = 1)\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n# plot\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\ncensus_features = ['Population_County_log','Drive_County','Transit_County_log','Walk_County_log','MedianHouseholdIncome_County']\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(census_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{}'.format(feature), size=16, y=1.05)\nfig.suptitle('Density of Accidents in Census Data (resampled data)', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"WxDxBNdzeIMK"},"cell_type":"markdown","source":"Percent of people taking transit to commute seems to related to severity. Level 4 accidents happened more frequently in those counties with a lower usage rate of transit.","execution_count":null},{"metadata":{"id":"C6xGWgUb29TQ"},"cell_type":"markdown","source":"### Street\nThere are more and more studies found that higher speed limits were associated with an increased likelihood of crashes and deaths. (https://www.cga.ct.gov/2013/rpt/2013-R-0074.htm) And speed limits are highly related to street type. Street type hence can be a good predictor of serious accidents. There is no feature about street type in the original dataset though, we can extract it from the street name. \n\nThe top 40 most common words in street names were selected. This list contains not only street types but also some common words widely used in street names.","execution_count":null},{"metadata":{"id":"OAB1njC3oLRg","outputId":"94a0f9fc-1dc9-44b0-d4ad-58b34eb474b0","trusted":true},"cell_type":"code","source":"# create a list of top 40 most common words in street name\nst_type =' '.join(df['Street'].unique().tolist()) # flat the array of street name\nst_type = re.split(\" |-\", st_type) # split the long string by space and hyphen\nst_type = [x[0] for x in Counter(st_type).most_common(40)] # select the 40 most common words\nprint('the 40 most common words')\nprint(*st_type, sep = \", \") \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove some irrelevant words and add spaces and hyphen back","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove some irrelevant words and add spaces and hyphen back\nst_type= [' Rd', ' St', ' Dr', ' Ave', ' Blvd', ' Ln', ' Highway', ' Pkwy', ' Hwy', \n          ' Way', ' Ct', 'Pl', ' Road', 'US-', 'Creek', ' Cir', 'Hill', 'Route', \n          'I-', 'Trl', 'Valley', 'Ridge', 'Pike', ' Fwy', 'River']\nprint(*st_type, sep = \", \")  ","execution_count":null,"outputs":[]},{"metadata":{"id":"3-V2Fl8KhJOX"},"cell_type":"markdown","source":"Create a dummy variable for each word in the list and plot the correlation between these key words and severity. ","execution_count":null},{"metadata":{"id":"vwyKBYPBtxxC","outputId":"8bc728b9-3f87-4247-b487-5e31471bcb6c","trusted":true},"cell_type":"code","source":"# for each word create a boolean column\nfor i in st_type:\n  df[i.strip()] = np.where(df['Street'].str.contains(i, case=True, na = False), 1, 0)\ndf.loc[df['Road']==1,'Rd'] = 1\ndf.loc[df['Highway']==1,'Hwy'] = 1\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\n# plot correlation\ndf_bl['Severity4'] = df_bl['Severity4'].astype(int)\nstreet_corr  = df_bl.loc[:,['Severity4']+[x.strip() for x in st_type]].corr()\nplt.figure(figsize=(20,15))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(street_corr, annot=True, cmap=cmap, center=0).set_title(\"Correlation (resampled data)\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ZX1pFj6w72RR"},"cell_type":"markdown","source":"Interstate Highway turns out to be the most dangerous street. Other roads like basic road, street, drive, and avenue are relatively safe. Let's just keep these five features.\n","execution_count":null},{"metadata":{"id":"ky0vP12s0vhN","trusted":true},"cell_type":"code","source":"drop_list = street_corr.index[street_corr['Severity4'].abs()<0.1].to_list()\ndf = df.drop(drop_list, axis=1)\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"E-g3oLbvi_xB"},"cell_type":"markdown","source":"### Side\nRight side of the line is much more dangerous than left side.","execution_count":null},{"metadata":{"id":"D6M3mOEunmol","outputId":"65a7895d-403b-4e32-bed0-13e56fe44e44","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nchart = sns.countplot(x='Side', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents by Side (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"vGeS2-xohXyv"},"cell_type":"markdown","source":"### Latitude and Longitude","execution_count":null},{"metadata":{"outputId":"4bc9ff3e-b6c0-408f-b713-1aba84132cc3","id":"zJm_JjwNL2LD","trusted":true},"cell_type":"code","source":"df_bl['Severity4'] = df_bl['Severity4'].astype('category')\nnum_features = ['Start_Lat', 'Start_Lng']\nfig, axs = plt.subplots(ncols=1, nrows=2, figsize=(10, 5))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(1, 2, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Feature'.format(feature), size=14, y=1.05)\nfig.suptitle('Distribution of Accidents by Latitude and Longitude\\n(resampled data)', fontsize=18,y=1.08)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"P1uG9VEA40SG","outputId":"2b8c5556-f7c9-49fb-b91f-ef3562f9ed6a","trusted":true},"cell_type":"code","source":"df_4 = df[df['Severity4']==1]\n\nplt.figure(figsize=(15,10))\n\nplt.plot( 'Start_Lng', 'Start_Lat', data=df, linestyle='', marker='o', markersize=1.5, color=\"teal\", alpha=0.2, label='All Accidents')\nplt.plot( 'Start_Lng', 'Start_Lat', data=df_4, linestyle='', marker='o', markersize=3, color=\"coral\", alpha=0.5, label='Accidents with Serverity Level 4')\nplt.legend(markerscale=8)\nplt.xlabel('Longitude', size=12, labelpad=3)\nplt.ylabel('Latitude', size=12, labelpad=3)\nplt.title('Map of Accidents', size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"L2VeGRLpNKFj"},"cell_type":"markdown","source":"### Frequency Encoding\nSimilar to 'Minute', some location features like 'City' and 'Zipcode' that have too many unique values can be labeled by their frequency. \nFrequency encoding and log-transform:\n1. 'Street'\n2. 'City'\n3. 'County'\n4. 'Zipcode'\n5. 'Airport_Code'","execution_count":null},{"metadata":{"id":"KAGDyEtnNKFm","trusted":true},"cell_type":"code","source":"fre_list = ['Street', 'City', 'County', 'Zipcode', 'Airport_Code']\nfor i in fre_list:\n  newname = i + '_Freq'\n  df[newname] = df.groupby([i])[i].transform('count')\n  df[newname] = df[newname]/df.shape[0]*df[i].unique().size\n  df[newname] = df[newname].apply(lambda x: np.log(x+1))","execution_count":null,"outputs":[]},{"metadata":{"id":"PEgYHZ1hT-qS","outputId":"edf3b769-c577-4cd0-92e1-2212a0bf81b6","trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(10, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfig.suptitle('Location Frequency by Severity (resampled data)', fontsize=16)\nfor i, feature in enumerate(fre_list, 1): \n    feature = feature + '_Freq'   \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity4', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{}'.format(feature), size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YOzbXDXoqA-X"},"cell_type":"markdown","source":"Two opposite patterns can be identified in these plots. For 'Street' and 'Zipcode', higher frequency means higher likelihood of being a serious accident. In contrast with these smaller regions, for 'City' and 'Airport_Code' instead, higher frequency means less likelihood of being a serious accident.\nGet rid of features we don't need anymore.","execution_count":null},{"metadata":{"id":"lxN5CdzbeSOM","trusted":true},"cell_type":"code","source":"df = df.drop(fre_list, axis  = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Dj2qNgYChWeV"},"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n## 3.4 Weather Features\n### Continuous Weather Features\nNormalize features with extreamly skewed distribution first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pressure_bc']= boxcox(df['Pressure(in)'].apply(lambda x: x+1),lmbda=6)\ndf['Visibility_bc']= boxcox(df['Visibility(mi)'].apply(lambda x: x+1),lmbda = 0.1)\ndf['Wind_Speed_bc']= boxcox(df['Wind_Speed(mph)'].apply(lambda x: x+1),lmbda=-0.2)\ndf = df.drop(['Pressure(in)','Visibility(mi)','Wind_Speed(mph)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"outputId":"d1275664-8b04-47a2-b8ce-c1ed1fb5673b","id":"1RZ5406FLsiO","trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nnum_features = ['Temperature(F)', 'Humidity(%)', 'Pressure_bc', 'Visibility_bc', 'Wind_Speed_bc']\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Feature by Severity'.format(feature), size=14, y=1.05)\nfig.suptitle('Density of Accidents by Weather Features (resampled data)', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"6xLsmH1WkWGf"},"cell_type":"markdown","source":"### Weather Conditions","execution_count":null},{"metadata":{"id":"q_WCDw0Hhi5z","outputId":"b1f3f1d1-ddfb-40e4-b699-835779717791","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.6)\nfor i, feature in enumerate(weather, 1):    \n    plt.subplot(2, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in \\n {} Feature'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents by Weather Features (resampled data)', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"6vUvGy06ukEa"},"cell_type":"markdown","source":"As seen from above, accidents are little more likely to be serious during rain or snow while less likely on a cloudy day. ","execution_count":null},{"metadata":{"id":"DUjTCMoDkbYJ"},"cell_type":"markdown","source":"### Wind Direction","execution_count":null},{"metadata":{"id":"XBp-XXbCkLIx","outputId":"ecd8e703-6af5-4b50-815b-fef3b2bca1aa","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nchart = sns.countplot(x='Wind_Direction', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents in Wind Direction (resample data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Qx0FIRMQ8Ot6"},"cell_type":"markdown","source":"<a id=\"3.5\"></a>\n## 3.5 POI Features","execution_count":null},{"metadata":{"id":"0XaRPpf9uMd8","outputId":"23fb12cd-7340-41c2-a222-bcfb2f598773","trusted":true},"cell_type":"code","source":"POI_features = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal']\n\nfig, axs = plt.subplots(ncols=3, nrows=4, figsize=(15, 10))\n\nplt.subplots_adjust(hspace=0.5,wspace = 0.5)\nfor i, feature in enumerate(POI_features, 1):    \n    plt.subplot(3, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in {}'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents in POI Features (resampled data)',y=1.02, fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"","execution_count":null},{"metadata":{"id":"nsgFVwAVxFwc"},"cell_type":"markdown","source":"Accidents near traffic signal and crossing are much less likely to be serious accidents while little more likely to be serious if they are near the junction. Maybe it is because people usually slow down in front of crossing and traffic signal but junction and severity are highly related to speed. Other POI features are so unbalanced that it is hard to tell their relation with severity from plots.\n\nDrop some features:\n\n1. 'Bump'\n2. 'Give_Way'\n3. 'No_Exit'\n4. 'Roundabout'\n5. 'Traffic_Calming'","execution_count":null},{"metadata":{"id":"NDbhqbFI3sF_","trusted":true},"cell_type":"code","source":"df= df.drop(['Bump','Give_Way','No_Exit','Roundabout','Traffic_Calming'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"r5620A5QdxG3"},"cell_type":"markdown","source":"<a id=\"3.6\"></a>\n## 3.6 Correlation\nFeatures are not highly correlated with each other. The top 3 highest correlations between severity and features are -0.17 (traffic_signal), 0.15 (Start_Lng), 0.12 (Start_Lat).","execution_count":null},{"metadata":{"id":"FdqM-0STlezN","outputId":"bc4caac4-4d86-46fa-e37b-30b394e4133a","trusted":true},"cell_type":"code","source":"# one-hot encoding\ndf[period_features] = df[period_features].astype('category')\ndf = pd.get_dummies(df, columns=period_features, drop_first=True)\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\n# plot correlation\ndf_bl['Severity4'] = df_bl['Severity4'].astype(int)\nplt.figure(figsize=(20,20))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(df_bl.corr(), annot=True,cmap=cmap, center=0).set_title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qei9EH-j0BAj"},"cell_type":"markdown","source":"The above figure shows strong positive correlations between 'Severity4' (level 4 severity) with 'I-' (Interstate highway) as well as 'Street_Freq', and strong negative correlation between it with 'Traffic_signal' and 'Minute_Freq'.\n\nWe can also identify several highly colinear features, such as 'Day'-'Month', 'Minute'-'Hour', 'Transit_County_log'-'Population_County_log', 'Airport_Code_Freq'-'City_Freq', and four period-of-day features. Let's drop some of them.   \n\ndrop features:\n1. 'Day'\n2. 'Minute'\n3. 'Population_County_log'\n4. 'City_Freq'\n5. 'Civil_Twilight_Night'\n6. 'Nautical_Twilight_Night'","execution_count":null},{"metadata":{"id":"A93f9cwgBRv-","trusted":true},"cell_type":"code","source":"df = df.drop(['Day','Minute','Population_County_log','City_Freq','Civil_Twilight_Night',\n              'Nautical_Twilight_Night'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"eqH6gMcPfQJ3"},"cell_type":"markdown","source":"<a id=\"3.7\"></a>\n## 3.7 One-hot Encoding \nOne-hot encode categorical features.","execution_count":null},{"metadata":{"id":"6FvjRW0piXup","trusted":true},"cell_type":"code","source":"df = df.replace([True, False], [1,0])\n\ncat = ['Side','State','Timezone','Wind_Direction', 'Weekday', 'Month', 'Hour']\ndf[cat] = df[cat].astype('category')\ndf = pd.get_dummies(df, columns=cat, drop_first=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"l2Fl3HBwZDSa"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# 4 Model\n<a id=\"4.1\"></a>\n## 4.1 Train Test Split\nResample data and split it into X and y.\n\nStandardize features based on unit variance.\n\nSplit data into X_train, X_test, y_train, and y_test. The size of training data is about 64000 and the test is about 16000.","execution_count":null},{"metadata":{"id":"M8azlFhkiN3w","trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n# split X, y\nX = df_bl.drop('Severity4', axis=1)\ny= df_bl['Severity4']\n\n# Standardizing the features based on unit variance\nfrom sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(X)\n\n# split train test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\\\n  X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"wa7H--dhkW4X"},"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n## 4.2 Logistic Regression\nLogistic regression was emploied as a baseline to perform binary classification task.","execution_count":null},{"metadata":{"id":"ImwhVzkHhHmg","outputId":"4cea6bad-ff27-4579-dcea-7f6691e397ae","trusted":true},"cell_type":"code","source":"# Logistic regression with default setting.\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(max_iter=10000,random_state=42)\nclf.fit(X_train, y_train)\n\naccuracy_train = clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\naccuracy_test = clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"mexi2Lnrh338","outputId":"20388eed-d944-4ff6-974c-36d1ea0b9467","trusted":true},"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix\n\ny_pred = clf.predict(X_test)\n\nconfmat = multilabel_confusion_matrix(y_true=y_test, y_pred=y_pred,\n                           labels=[1])\n\nconf_matrix = pd.DataFrame(data=confmat[0],\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix (resampled data)\\n-Default Logistic Regression\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"IL-QsuIhMBWt"},"cell_type":"markdown","source":"The grid search was performed over choices of 'penalty': {'none','l2'}, 'C': {0.001,.009,0.01,.09,1,5,10,25}, 'maximum iterations': {1000, 10000, 100000}","execution_count":null},{"metadata":{"id":"qwHZcN445RZY","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#Grid Search\nfrom sklearn.model_selection import GridSearchCV\nLR_grid = {\n           'penalty':            ['none','l2'],\n           'C':                  [0.001,.009,0.01,.09,1,5,10,25],\n           'max_iter': [1000, 10000, 100000]\n           }\nCV_LR = GridSearchCV(estimator=LogisticRegression(random_state=42), param_grid = LR_grid,scoring = 'accuracy',cv=5)\nCV_LR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Parameters: ', CV_LR.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"3x0wIvLJmLZA","outputId":"16d01874-8588-4351-f76c-2e22e3023f54","trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nCV_LR_clf = LogisticRegression(C=0.09, max_iter=1000, penalty='l2')\nCV_LR_clf.fit(X_train, y_train)\naccuracy_train = CV_LR_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\naccuracy_test = CV_LR.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"KMe3yjUqLIy_"},"cell_type":"markdown","source":"Even with the best parameter setting, logistic regression yielded very poor results for both training data and test data.","execution_count":null},{"metadata":{"id":"7yTc8zWgiQOs"},"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n## 4.3 Decision Tree\nThen, decision tree classifier was employied. The grid search was performed over choices of 'min_samples_split': {5,10, 20, 30, 40}, 'max_features': {None, 'log2', 'sqrt'}","execution_count":null},{"metadata":{"id":"-iYnKSIV_OZY","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nDT_grid = { 'min_samples_split': [5,10, 20, 30, 40], \n          'max_features': [None, 'log2', 'sqrt']}\nCV_DT = GridSearchCV(DecisionTreeClassifier(random_state=42), DT_grid, verbose=1, cv=3)\nCV_DT.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ln4Ke5qSAp7v","outputId":"73acea06-b31e-4a11-9fff-a6c25e150b03","trusted":true},"cell_type":"code","source":"print('Best Parameters: ', CV_DT.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"BbxJ0osqidKN","outputId":"d173cb87-7a07-4341-cdde-479eea01ca2b","trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn import tree\n# Training step, on X_train with y_train\ntree_clf = tree.DecisionTreeClassifier(min_samples_split = 5)\ntree_clf = tree_clf.fit(X_train,y_train)\n\ntree_accuracy_train = tree_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (tree_accuracy_train*100))\ntree_accuracy_test = tree_clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (tree_accuracy_test*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"UCJOSMgIrPi-","outputId":"865a1fde-ecbb-4aff-e858-f62fe79822fd","trusted":true},"cell_type":"code","source":"prediction = tree_clf.predict(X_test)\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=prediction)\n\nconf_matrix = pd.DataFrame(data=confmat,\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix (resampled data)\\n Decision Tree\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"3qouHFVcOFhP"},"cell_type":"markdown","source":"By using decision tree classifier, the training accuracy improved to 99.8% and test accuracy to 96.0%. The result is almost perfect.","execution_count":null},{"metadata":{"id":"wc1j7Lpp9oEW","outputId":"ed450939-aca6-4fbb-bcd4-224c167d34d1","trusted":true},"cell_type":"code","source":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], \n                           index=df.drop('Severity4',axis=1).columns)\n\nimportances.iloc[:,0] = tree_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportances30 = importances.head(30)\n\nplt.figure(figsize=(10, 10))\nsns.barplot(x='importance', y=importances30.index, data=importances30)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Decision Tree Classifier Feature Importance', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"w42dA7ZSO_oV"},"cell_type":"markdown","source":"The feature importance plot shows that high-resolution spatio-temporal patterns of accidents are the most useful features to predict severity. Among them, *street frequency* is far more important than any other feature. In addition to these spatio-temporal features, weather features like *pressure*, *temperature*, *humidity*, and *wind speed* are also very important. Some other features like *interstate highway*('I-'), *traffic signal* are important as well.","execution_count":null},{"metadata":{"id":"qpj8zlDCBwoP","outputId":"a1517703-3681-4e09-a9ac-067a6e5dcb9e","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\ntree.plot_tree(tree_clf, max_depth=4, fontsize=10,\n               feature_names=df.drop('Severity4',axis =1).columns.to_list(),\n               class_names = True, filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"JBprZxAzorJD"},"cell_type":"markdown","source":"<a id=\"4.4\"></a>\n## 4.4 Random Forest\nFinally, random forest classifier was employied. The grid search was performed over choices of 'n_estimators': {30,40,50}, 'max_depth': {20,30,40}.","execution_count":null},{"metadata":{"id":"i3RTSLOY9RcC","outputId":"7106c7e6-0542-4f92-dcb6-838cf3746193","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = { \n    'n_estimators'     : [30,40,50],\n    'max_depth'        : [20,30,40]\n}\nCV_clf = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid,cv=4)\nCV_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Parameters: ', CV_clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"lnRq4Z5dzGay","outputId":"0348e1c8-111b-41d0-b009-5ab4f8646b32","trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\nrf_clf = RandomForestRegressor(max_depth=40,n_estimators=50)\nrf_clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"99AWl8-QWUnk","outputId":"9852d80d-e6bf-4a27-acb6-465889abaad0","trusted":true},"cell_type":"code","source":"f = lambda x: 1 if x>=0.5 else 0\ntrain_pred = np.array(list(map(f, rf_clf.predict(X_train))))\ntest_pred = np.array(list(map(f, rf_clf.predict(X_test))))\n\nrf_train_accuracy = accuracy_score(y_train, train_pred)\nprint(\"Train Accuracy: %.1f%%\"% (rf_train_accuracy*100))\nrf_test_accuracy = accuracy_score(y_test, test_pred)\nprint(\"Test Accuracy: %.1f%%\"% (rf_test_accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"rJbjZAeN99ky","outputId":"e63c0c83-fac1-4108-cafa-5cd4696bf3aa","trusted":true},"cell_type":"code","source":"confmat = confusion_matrix(y_true=y_test, y_pred=test_pred)\n\nconf_matrix = pd.DataFrame(data=confmat,\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix (resampled data)\\n Random Forest\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"cCx8FAUrULn9"},"cell_type":"markdown","source":"By using random forest classifier, the model achieved 100.0% train accuracy and 97.9% test accuracy, which is even better than the results of decision tree classifier. But it took a much longer time to train.   ","execution_count":null},{"metadata":{"id":"3YbCHBm32eq_","outputId":"20f9cc44-546a-44b3-dc48-a7d182ead093","trusted":true},"cell_type":"code","source":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], index=df.drop('Severity4',axis=1).columns)\n\nimportances.iloc[:,0] = rf_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportances30 = importances.head(30)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='importance', y=importances30.index, data=importances30)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Random Forest Classifier Feature Importance', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QeubBJZyYSX-"},"cell_type":"markdown","source":"The top 15 important features of random forest model are almost as same as decision tree model.","execution_count":null},{"metadata":{"id":"R-rC-g9Bwrp8"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# 5 Future Work\n1. Incorporate this model in a real-time accident risk prediction model or develop a new real-time severe accident risk prediction on grid cells.\n2. Detailed relations between some key factors and accident severity can be further studied.\n3. Policy implications of this project can be explored.  \n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}